{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97bc449",
   "metadata": {},
   "source": [
    "Link Video : https://binusianorg-my.sharepoint.com/personal/arvio_anandi_binus_ac_id/_layouts/15/guestaccess.aspx?docid=0b721ef4b383c4941a1e4d914ec005afe&authkey=AVMqHBfdOhp0cojOhm7vlik&e=jaTrsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1afc4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import skew\n",
    "import statsmodels.tsa.stattools as sts\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError \n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsolutePercentageError\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "import scipy.stats as stats\n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01174f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-12-12</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.406782</td>\n",
       "      <td>117258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-12-15</td>\n",
       "      <td>0.488839</td>\n",
       "      <td>0.488839</td>\n",
       "      <td>0.486607</td>\n",
       "      <td>0.486607</td>\n",
       "      <td>0.385558</td>\n",
       "      <td>43971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-12-16</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>0.357260</td>\n",
       "      <td>26432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-12-17</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>0.366103</td>\n",
       "      <td>21610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-12-18</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>0.477679</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>0.376715</td>\n",
       "      <td>18362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1980-12-19</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>0.399707</td>\n",
       "      <td>12157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1980-12-22</td>\n",
       "      <td>0.529018</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.529018</td>\n",
       "      <td>0.529018</td>\n",
       "      <td>0.419162</td>\n",
       "      <td>9340800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1980-12-23</td>\n",
       "      <td>0.551339</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.551339</td>\n",
       "      <td>0.551339</td>\n",
       "      <td>0.436848</td>\n",
       "      <td>11737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1980-12-24</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.582589</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.459840</td>\n",
       "      <td>12000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1980-12-26</td>\n",
       "      <td>0.633929</td>\n",
       "      <td>0.636161</td>\n",
       "      <td>0.633929</td>\n",
       "      <td>0.633929</td>\n",
       "      <td>0.502287</td>\n",
       "      <td>13893600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close     Volume\n",
       "0  1980-12-12  0.513393  0.515625  0.513393  0.513393   0.406782  117258400\n",
       "1  1980-12-15  0.488839  0.488839  0.486607  0.486607   0.385558   43971200\n",
       "2  1980-12-16  0.453125  0.453125  0.450893  0.450893   0.357260   26432000\n",
       "3  1980-12-17  0.462054  0.464286  0.462054  0.462054   0.366103   21610400\n",
       "4  1980-12-18  0.475446  0.477679  0.475446  0.475446   0.376715   18362400\n",
       "5  1980-12-19  0.504464  0.506696  0.504464  0.504464   0.399707   12157600\n",
       "6  1980-12-22  0.529018  0.531250  0.529018  0.529018   0.419162    9340800\n",
       "7  1980-12-23  0.551339  0.553571  0.551339  0.551339   0.436848   11737600\n",
       "8  1980-12-24  0.580357  0.582589  0.580357  0.580357   0.459840   12000800\n",
       "9  1980-12-26  0.633929  0.636161  0.633929  0.633929   0.502287   13893600"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baca dataset dari file CSV\n",
    "df = pd.read_csv('C://Users/User/Downloads/UAS_DL/AAPL.csv')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f55c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9909 entries, 0 to 9908\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       9909 non-null   object \n",
      " 1   Open       9909 non-null   float64\n",
      " 2   High       9909 non-null   float64\n",
      " 3   Low        9909 non-null   float64\n",
      " 4   Close      9909 non-null   float64\n",
      " 5   Adj Close  9909 non-null   float64\n",
      " 6   Volume     9909 non-null   int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 542.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9f3b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9909.000000</td>\n",
       "      <td>9909.000000</td>\n",
       "      <td>9909.000000</td>\n",
       "      <td>9909.000000</td>\n",
       "      <td>9909.000000</td>\n",
       "      <td>9.909000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.606849</td>\n",
       "      <td>32.936079</td>\n",
       "      <td>32.277560</td>\n",
       "      <td>32.618030</td>\n",
       "      <td>30.576570</td>\n",
       "      <td>8.582916e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>58.415759</td>\n",
       "      <td>59.001576</td>\n",
       "      <td>57.883037</td>\n",
       "      <td>58.471899</td>\n",
       "      <td>56.746275</td>\n",
       "      <td>8.597195e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.198661</td>\n",
       "      <td>0.198661</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.155638</td>\n",
       "      <td>3.472000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.089286</td>\n",
       "      <td>1.048571</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>0.917643</td>\n",
       "      <td>3.304230e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.729286</td>\n",
       "      <td>1.758929</td>\n",
       "      <td>1.696429</td>\n",
       "      <td>1.732143</td>\n",
       "      <td>1.466154</td>\n",
       "      <td>5.766490e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.799999</td>\n",
       "      <td>36.265713</td>\n",
       "      <td>35.328571</td>\n",
       "      <td>35.761429</td>\n",
       "      <td>31.042374</td>\n",
       "      <td>1.069992e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>324.739990</td>\n",
       "      <td>327.850006</td>\n",
       "      <td>323.350006</td>\n",
       "      <td>327.200012</td>\n",
       "      <td>327.200012</td>\n",
       "      <td>1.855410e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close    Adj Close  \\\n",
       "count  9909.000000  9909.000000  9909.000000  9909.000000  9909.000000   \n",
       "mean     32.606849    32.936079    32.277560    32.618030    30.576570   \n",
       "std      58.415759    59.001576    57.883037    58.471899    56.746275   \n",
       "min       0.198661     0.198661     0.196429     0.196429     0.155638   \n",
       "25%       1.071429     1.089286     1.048571     1.071429     0.917643   \n",
       "50%       1.729286     1.758929     1.696429     1.732143     1.466154   \n",
       "75%      35.799999    36.265713    35.328571    35.761429    31.042374   \n",
       "max     324.739990   327.850006   323.350006   327.200012   327.200012   \n",
       "\n",
       "             Volume  \n",
       "count  9.909000e+03  \n",
       "mean   8.582916e+07  \n",
       "std    8.597195e+07  \n",
       "min    3.472000e+05  \n",
       "25%    3.304230e+07  \n",
       "50%    5.766490e+07  \n",
       "75%    1.069992e+08  \n",
       "max    1.855410e+09  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03809aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5769e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.drop(['Open','High','Low','Adj Close','Volume'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b21e285a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-12-12</td>\n",
       "      <td>0.513393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-12-15</td>\n",
       "      <td>0.486607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-12-16</td>\n",
       "      <td>0.450893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-12-17</td>\n",
       "      <td>0.462054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-12-18</td>\n",
       "      <td>0.475446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>258.440002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>247.740005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>254.809998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>254.289993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>240.910004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9909 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Close\n",
       "0     1980-12-12    0.513393\n",
       "1     1980-12-15    0.486607\n",
       "2     1980-12-16    0.450893\n",
       "3     1980-12-17    0.462054\n",
       "4     1980-12-18    0.475446\n",
       "...          ...         ...\n",
       "9904  2020-03-26  258.440002\n",
       "9905  2020-03-27  247.740005\n",
       "9906  2020-03-30  254.809998\n",
       "9907  2020-03-31  254.289993\n",
       "9908  2020-04-01  240.910004\n",
       "\n",
       "[9909 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f18d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.index = pd.to_datetime(target.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9384e885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Date'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db8cf80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime type\n",
    "target['Date'] = pd.to_datetime(target['Date'])\n",
    "\n",
    "# Get the start of the week for each date\n",
    "target['WeekStart'] = target['Date'] - pd.to_timedelta(target['Date'].dt.dayofweek, unit='d')\n",
    "\n",
    "# Group by week and count the number of unique dates in each week\n",
    "week_counts = target.groupby('WeekStart')['Date'].nunique()\n",
    "\n",
    "# Find the weeks with 5 unique dates (complete weeks)\n",
    "complete_weeks = week_counts[week_counts == 5].index\n",
    "\n",
    "# Filter the DataFrame to keep only the complete weeks\n",
    "cleaned_df = target[target['WeekStart'].isin(complete_weeks)]\n",
    "\n",
    "# Reset the index if desired\n",
    "cleaned_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ed365",
   "metadata": {},
   "source": [
    "To handle missing dates in the dataset, we create a new column called WeekStart which is the replication of Date column, then we find the week with 5 unique dates (complete weeks), and lastly we filter the dataframe to keep only the complete weeks and reset the index to get a new dataframe with only complete weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6324bf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>WeekStart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-12-15</td>\n",
       "      <td>0.486607</td>\n",
       "      <td>1980-12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-12-16</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>1980-12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-12-17</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>1980-12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-12-18</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>1980-12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-12-19</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>1980-12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1981-01-05</td>\n",
       "      <td>0.602679</td>\n",
       "      <td>1981-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1981-01-06</td>\n",
       "      <td>0.575893</td>\n",
       "      <td>1981-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1981-01-07</td>\n",
       "      <td>0.551339</td>\n",
       "      <td>1981-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1981-01-08</td>\n",
       "      <td>0.540179</td>\n",
       "      <td>1981-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1981-01-09</td>\n",
       "      <td>0.569196</td>\n",
       "      <td>1981-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1981-01-12</td>\n",
       "      <td>0.564732</td>\n",
       "      <td>1981-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1981-01-13</td>\n",
       "      <td>0.544643</td>\n",
       "      <td>1981-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1981-01-14</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>1981-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1981-01-15</td>\n",
       "      <td>0.558036</td>\n",
       "      <td>1981-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1981-01-16</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>1981-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1981-01-19</td>\n",
       "      <td>0.587054</td>\n",
       "      <td>1981-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1981-01-20</td>\n",
       "      <td>0.569196</td>\n",
       "      <td>1981-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1981-01-21</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>1981-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1981-01-22</td>\n",
       "      <td>0.587054</td>\n",
       "      <td>1981-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1981-01-23</td>\n",
       "      <td>0.584821</td>\n",
       "      <td>1981-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1981-01-26</td>\n",
       "      <td>0.575893</td>\n",
       "      <td>1981-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1981-01-27</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1981-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1981-01-28</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>1981-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1981-01-29</td>\n",
       "      <td>0.533482</td>\n",
       "      <td>1981-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1981-01-30</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>1981-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1981-02-02</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>1981-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1981-02-03</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>1981-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1981-02-04</td>\n",
       "      <td>0.511161</td>\n",
       "      <td>1981-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1981-02-05</td>\n",
       "      <td>0.511161</td>\n",
       "      <td>1981-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1981-02-06</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>1981-02-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Close  WeekStart\n",
       "0  1980-12-15  0.486607 1980-12-15\n",
       "1  1980-12-16  0.450893 1980-12-15\n",
       "2  1980-12-17  0.462054 1980-12-15\n",
       "3  1980-12-18  0.475446 1980-12-15\n",
       "4  1980-12-19  0.504464 1980-12-15\n",
       "5  1981-01-05  0.602679 1981-01-05\n",
       "6  1981-01-06  0.575893 1981-01-05\n",
       "7  1981-01-07  0.551339 1981-01-05\n",
       "8  1981-01-08  0.540179 1981-01-05\n",
       "9  1981-01-09  0.569196 1981-01-05\n",
       "10 1981-01-12  0.564732 1981-01-12\n",
       "11 1981-01-13  0.544643 1981-01-12\n",
       "12 1981-01-14  0.546875 1981-01-12\n",
       "13 1981-01-15  0.558036 1981-01-12\n",
       "14 1981-01-16  0.553571 1981-01-12\n",
       "15 1981-01-19  0.587054 1981-01-19\n",
       "16 1981-01-20  0.569196 1981-01-19\n",
       "17 1981-01-21  0.580357 1981-01-19\n",
       "18 1981-01-22  0.587054 1981-01-19\n",
       "19 1981-01-23  0.584821 1981-01-19\n",
       "20 1981-01-26  0.575893 1981-01-26\n",
       "21 1981-01-27  0.571429 1981-01-26\n",
       "22 1981-01-28  0.553571 1981-01-26\n",
       "23 1981-01-29  0.533482 1981-01-26\n",
       "24 1981-01-30  0.504464 1981-01-26\n",
       "25 1981-02-02  0.475446 1981-02-02\n",
       "26 1981-02-03  0.493304 1981-02-02\n",
       "27 1981-02-04  0.511161 1981-02-02\n",
       "28 1981-02-05  0.511161 1981-02-02\n",
       "29 1981-02-06  0.513393 1981-02-02"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the resulting DataFrame\n",
    "cleaned_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2287565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = cleaned_df.drop(['WeekStart'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c44274",
   "metadata": {},
   "source": [
    "Filter the dataset from unused column WeekStart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3ef100",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.index = pd.to_datetime(target.Date)\n",
    "target = target.drop(['Date'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e30db",
   "metadata": {},
   "source": [
    "We set the target Date as the target index and we remove the unused Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c3e3236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-12-15</th>\n",
       "      <td>0.486607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-16</th>\n",
       "      <td>0.450893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-17</th>\n",
       "      <td>0.462054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-18</th>\n",
       "      <td>0.475446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-19</th>\n",
       "      <td>0.504464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>0.602679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-06</th>\n",
       "      <td>0.575893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-07</th>\n",
       "      <td>0.551339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-08</th>\n",
       "      <td>0.540179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-09</th>\n",
       "      <td>0.569196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-12</th>\n",
       "      <td>0.564732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-13</th>\n",
       "      <td>0.544643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-14</th>\n",
       "      <td>0.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-15</th>\n",
       "      <td>0.558036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-16</th>\n",
       "      <td>0.553571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-19</th>\n",
       "      <td>0.587054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-20</th>\n",
       "      <td>0.569196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-21</th>\n",
       "      <td>0.580357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-22</th>\n",
       "      <td>0.587054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-23</th>\n",
       "      <td>0.584821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Close\n",
       "Date                \n",
       "1980-12-15  0.486607\n",
       "1980-12-16  0.450893\n",
       "1980-12-17  0.462054\n",
       "1980-12-18  0.475446\n",
       "1980-12-19  0.504464\n",
       "1981-01-05  0.602679\n",
       "1981-01-06  0.575893\n",
       "1981-01-07  0.551339\n",
       "1981-01-08  0.540179\n",
       "1981-01-09  0.569196\n",
       "1981-01-12  0.564732\n",
       "1981-01-13  0.544643\n",
       "1981-01-14  0.546875\n",
       "1981-01-15  0.558036\n",
       "1981-01-16  0.553571\n",
       "1981-01-19  0.587054\n",
       "1981-01-20  0.569196\n",
       "1981-01-21  0.580357\n",
       "1981-01-22  0.587054\n",
       "1981-01-23  0.584821"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9fd3b",
   "metadata": {},
   "source": [
    "## Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06d7bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.872676116150048,\n",
       " 1.0,\n",
       " 37,\n",
       " 8512,\n",
       " {'1%': -3.431118476637194,\n",
       "  '5%': -2.861879614422937,\n",
       "  '10%': -2.566950771852056},\n",
       " 29132.40439909454)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.adfuller(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701ec7a",
   "metadata": {},
   "source": [
    "The t-statistics value is greater than the 1%, 5%, and 10% critical values from the Dicky-Fuller table, which means for all level of these significance, there is no sufficient evidence of stationarity in the dataset. However, by looking at the p-value, it is certain that the data comes from a non-stationary process therefore, detrending methods such as differencing or fitting a regression model and subtracting the fitted values are needed for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12151ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is stored in a DataFrame called 'df' with a column named 'Value'\n",
    "\n",
    "# Log transformation\n",
    "target['Close'] = np.log(target['Close'])\n",
    "\n",
    "# Perform differencing on the log-transformed data\n",
    "target['Close'] = target['Close'].diff()\n",
    "target = target.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a8739",
   "metadata": {},
   "source": [
    "We handle the non-stationarity in the dataset by applying the log transformation and differencing method on the data, and drop the missing values created by the differencing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d79060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-45.9095401230632,\n",
       " 0.0,\n",
       " 3,\n",
       " 8545,\n",
       " {'1%': -3.4311155079592166,\n",
       "  '5%': -2.8618783026381287,\n",
       "  '10%': -2.5669500735787305},\n",
       " -34819.45022280677)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.adfuller(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac0988",
   "metadata": {},
   "source": [
    "The dataset p-value indicates that the dataset comes from a stationarity process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d6db1",
   "metadata": {},
   "source": [
    "## Window Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6099292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(df,window_size=5):\n",
    "    df_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0,len(df_np)-window_size,5):\n",
    "        row = [[a] for a in df_np[i:i+5]]\n",
    "        col = [[a] for a in df_np[i+5:i+10]]\n",
    "        X.append(row)\n",
    "        y.append(col)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8e96b",
   "metadata": {},
   "source": [
    "We partition the data by iterating through the dataset with timestep of 5, where in each iteration, we assign the first 5 values to the row variable, and the next 5 values to the col variable, then we append these values to an empty list of X and y. After the iterations, we return the X and y values as numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecea7bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1709, 5, 1, 1), (1709,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "X, y = window_partition(target, WINDOW_SIZE)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c3235",
   "metadata": {},
   "source": [
    "We apply the function to the dataset with window size of 5, and we print out the shape resulting the shape shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5ff9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X[:-1]\n",
    "y= y[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04992b8",
   "metadata": {},
   "source": [
    "We remove the last data from X and y as they turned out to be incomplete values after checking on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f14842e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1708, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape 'x' to (1708, 5)\n",
    "X = X.reshape(1708, 5)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771e8f0",
   "metadata": {},
   "source": [
    "We reshape the data to the format of (None,5) to fit the model input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a604c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "rsh= lambda y:np.array(y).reshape(5)\n",
    "for i in range(len(y)):\n",
    "    l.append(rsh(y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133e259",
   "metadata": {},
   "source": [
    "We also reshape the y data by reshaping through the iterations and appending each reshaped values to the list variable called l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b9ea176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1708, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(l)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f38e45",
   "metadata": {},
   "source": [
    "We transform the data to numpy and we print out the shape of y with results as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c591e",
   "metadata": {},
   "source": [
    "The data is reshaped and now fit for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abda95",
   "metadata": {},
   "source": [
    "## Data Splitting (80% Train, 10% Test, 10% Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "079f1b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1366, 5), (1366, 5), (170, 5), (170, 5), (172, 5), (172, 5))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(X)*0.8)\n",
    "val_size = int(len(X)*0.1)\n",
    "\n",
    "X_train, y_train = X[:train_size],y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:],y[train_size+val_size:]\n",
    "X_train.shape, y_train.shape,X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f65e6",
   "metadata": {},
   "source": [
    "The dataset is split into 80% training data, 10% testing data, and 10% validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00a89e",
   "metadata": {},
   "source": [
    "Short summary for preprocessing and exploration part : Overall, the dataset requires some preprocessing and exploration to be done such as handling non-stationarity, handling missing values, handling missing dates in the dataset, window partitioning, and splitting. Therefore, we can proceed to next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf696a",
   "metadata": {},
   "source": [
    "# 1.b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe03b8",
   "metadata": {},
   "source": [
    "## 1st Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e93ab860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 5, 1, 128)    1280000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 5, 128)       0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 128)       0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 5, 128)       0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 5, 128)      66048       ['add[0][0]',                    \n",
      " dAttention)                                                      'add[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 5, 128)      0           ['multi_head_attention[0][0]',   \n",
      " da)                                                              'add[0][0]']                    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 5, 128)       0           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 5, 128)      256         ['dropout_1[0][0]']              \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 5, 128)       0           ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 5, 1)         129         ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 5, 128)      0           ['conv1d[0][0]',                 \n",
      " mbda)                                                            'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['tf.__operators__.add_1[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 128)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 128)         256         ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            645         ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,347,334\n",
      "Trainable params: 1,347,334\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = 1367\n",
    "\n",
    "# Set the input shape\n",
    "input_shape = (5, 1)\n",
    "\n",
    "# Define the model architecture\n",
    "def create_transformer_model(input_shape, num_classes):\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x= inputs\n",
    "    \n",
    "    # Feature embedding\n",
    "    x = layers.Embedding(input_dim=10000, output_dim=128)(inputs)\n",
    "    x = Reshape(target_shape=(5,128))(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    # Positional embedding\n",
    "    positions = tf.range(start=0, limit=input_shape[1], delta=1)\n",
    "    positions = layers.Embedding(input_dim=input_shape[1], output_dim=128)(positions)\n",
    "\n",
    "    \n",
    "    # Add positional embedding to the input\n",
    "    x = layers.Add()([x, positions])\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    skip = x\n",
    "    x = layers.MultiHeadAttention(num_heads=8, key_dim=16)(x, x)\n",
    "    x+=skip\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    skip = x\n",
    "    # Reshape the input for Conv1D\n",
    "    x = layers.Reshape(target_shape=(input_shape[0], -1))(x)\n",
    "\n",
    "    # Feed Forward using 1 layer of Conv1D with activation ReLU\n",
    "    x = layers.Conv1D(filters=1, kernel_size=1, activation=\"relu\")(x)\n",
    "    x+=skip\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Take the last time step to create linear output\n",
    "    x = layers.Dense(units=5)(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Create the transformer model\n",
    "model1 = create_transformer_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer=Adam(learning_rate=0.0001), loss=MeanSquaredError(), metrics=[RootMeanSquaredError()])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81b0bd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 7s 63ms/step - loss: 0.3678 - root_mean_squared_error: 0.6065 - val_loss: 0.0050 - val_root_mean_squared_error: 0.0707\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.2406 - root_mean_squared_error: 0.4905 - val_loss: 0.0013 - val_root_mean_squared_error: 0.0366\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2291 - root_mean_squared_error: 0.4787 - val_loss: 0.0022 - val_root_mean_squared_error: 0.0470\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.2147 - root_mean_squared_error: 0.4634 - val_loss: 0.0032 - val_root_mean_squared_error: 0.0569\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 0.1995 - root_mean_squared_error: 0.4466 - val_loss: 0.0020 - val_root_mean_squared_error: 0.0444\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 0.1696 - root_mean_squared_error: 0.4119 - val_loss: 0.0022 - val_root_mean_squared_error: 0.0471\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 2s 43ms/step - loss: 0.1614 - root_mean_squared_error: 0.4018 - val_loss: 0.0016 - val_root_mean_squared_error: 0.0405\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 1s 32ms/step - loss: 0.1438 - root_mean_squared_error: 0.3792 - val_loss: 0.0044 - val_root_mean_squared_error: 0.0663\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 0.1383 - root_mean_squared_error: 0.3719 - val_loss: 9.4733e-04 - val_root_mean_squared_error: 0.0308\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 1s 31ms/step - loss: 0.1263 - root_mean_squared_error: 0.3554 - val_loss: 0.0020 - val_root_mean_squared_error: 0.0449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da8cfbd760>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model1.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da01d4a",
   "metadata": {},
   "source": [
    "# 1.c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70cfce",
   "metadata": {},
   "source": [
    "## 2nd Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95bc0d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 5, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 5, 1)        897         ['input_2[0][0]',                \n",
      " eadAttention)                                                    'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 5, 1)        0           ['multi_head_attention_1[0][0]', \n",
      " mbda)                                                            'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5, 1)         0           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 5, 1)        2           ['dropout_3[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 5, 1)         0           ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 5, 16)        32          ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 5, 16)        272         ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 5, 16)        272         ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 5, 16)       0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 5, 50)        13400       ['tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 5, 50)        20200       ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 50)           20200       ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 50)           0           ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 50)           0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 50)          100         ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           816         ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8)            136         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4)            36          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 5)            25          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 56,388\n",
      "Trainable params: 56,388\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = 1367\n",
    "\n",
    "# Set the input shape\n",
    "input_shape = (5, 1)\n",
    "\n",
    "# Define the model architecture\n",
    "def create_transformer_model(input_shape, num_classes):\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x= inputs\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    skip = x\n",
    "    x = layers.MultiHeadAttention(num_heads=8, key_dim=16)(x, x)\n",
    "    x+=skip\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    skip = x\n",
    "    # Reshape the input for Conv1D\n",
    "    x = layers.Reshape(target_shape=(input_shape[0], -1))(x)\n",
    "\n",
    "    # Feed Forward using 1 layer of Conv1D with activation ReLU\n",
    "    x = layers.Conv1D(filters=16, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters=16, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters=16, kernel_size=1, activation=\"relu\")(x)\n",
    "    x+=skip\n",
    "    x = LSTM(50,return_sequences=True)(x)\n",
    "    x = LSTM(50,return_sequences=True)(x)\n",
    "    x = LSTM(50)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Take the last time step to create linear output\n",
    "    x = layers.Dense(units=16)(x)\n",
    "    x = layers.Dense(units=8)(x)\n",
    "    x = layers.Dense(units=4)(x)\n",
    "    x = layers.Dense(units=5)(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Create the transformer model\n",
    "model2 = create_transformer_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer=Adam(learning_rate=0.003), loss=\"mae\", metrics=[RootMeanSquaredError()])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2911ebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 10s 42ms/step - loss: 0.3620 - root_mean_squared_error: 0.8994 - val_loss: 0.0746 - val_root_mean_squared_error: 0.0819\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0688 - root_mean_squared_error: 0.0962 - val_loss: 0.0483 - val_root_mean_squared_error: 0.0549\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0401 - root_mean_squared_error: 0.0546 - val_loss: 0.0176 - val_root_mean_squared_error: 0.0235\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0286 - root_mean_squared_error: 0.0392 - val_loss: 0.0172 - val_root_mean_squared_error: 0.0229\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0261 - root_mean_squared_error: 0.0367 - val_loss: 0.0144 - val_root_mean_squared_error: 0.0198\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0276 - root_mean_squared_error: 0.0381 - val_loss: 0.0168 - val_root_mean_squared_error: 0.0224\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0250 - root_mean_squared_error: 0.0358 - val_loss: 0.0140 - val_root_mean_squared_error: 0.0195\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0254 - root_mean_squared_error: 0.0360 - val_loss: 0.0145 - val_root_mean_squared_error: 0.0206\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0264 - root_mean_squared_error: 0.0371 - val_loss: 0.0160 - val_root_mean_squared_error: 0.0214\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0242 - root_mean_squared_error: 0.0348 - val_loss: 0.0146 - val_root_mean_squared_error: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da8e3662e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model2.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07963281",
   "metadata": {},
   "source": [
    "The model begins with an input layer that takes in (5, 1) as the input shape. This sets the foundation for processing the sequential data.\n",
    "\n",
    "Next, we incorporate a crucial component of Multi-Head Attention. This mechanism allows the model to attend to different parts of the input simultaneously, enhancing its ability to capture meaningful relationships. By leveraging eight attention heads and key vectors of dimension 16, our model can effectively process complex patterns within the data.\n",
    "\n",
    "To augment the learning process, we employ a skip connection, preserving the original input. This connection helps alleviate potential information loss during training. We apply dropout regularization to prevent overfitting and ensure the model's generalization capability.\n",
    "\n",
    "The subsequent step involves reshaping the input to facilitate the use of Conv1D layers. These layers, characterized by a kernel size of 1 and ReLU activation, further capture relevant features within the data. We introduce another skip connection, adding the intermediate results to the original input. This technique encourages the model to learn from both low-level and high-level representations simultaneously.\n",
    "\n",
    "To capture long-term dependencies and temporal patterns, we incorporate LSTM layers. These recurrent layers with 50 units each allow the model to understand the sequential nature of the data. By stacking multiple LSTM layers, we enable the model to learn hierarchical representations and extract meaningful insights from the sequences.\n",
    "\n",
    "To prepare the data for further processing, we flatten the output of the LSTM layers. This transformation converts the multidimensional output into a suitable format for subsequent layers. Dropout regularization and layer normalization are then applied to enhance model performance and stabilize training.\n",
    "\n",
    "The final part of our architecture consists of several dense layers. These layers gradually reduce the dimensionality of the data and introduce non-linearity. The number of units in the last dense layer is set to match the desired number of output classes, ensuring compatibility with the classification task.\n",
    "\n",
    "In summary, our transformer-based model architecture combines multi-head attention, Conv1D layers, LSTM layers, and dense layers to effectively capture and understand sequential data. By incorporating skip connections, dropout regularization, and layer normalization, we encourage robust learning and mitigate potential issues during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b58bc8",
   "metadata": {},
   "source": [
    "# 1.d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5494c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbec1e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "model1_predictions = model1.predict(X_test)\n",
    "model2_predictions = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17f22546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "model1_rmse = np.sqrt(mean_squared_error(model1_predictions,y_test))\n",
    "model2_rmse = np.sqrt(mean_squared_error(model2_predictions,y_test))\n",
    "model1_mae = mean_absolute_error(model1_predictions,y_test)\n",
    "model2_mae = mean_absolute_error(model2_predictions,y_test)\n",
    "model1_mape = mean_absolute_percentage_error(model1_predictions,y_test)\n",
    "model2_mape = mean_absolute_percentage_error(model2_predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79770af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           RMSE     MAE    MAPE\n",
      "Model                          \n",
      "Model 1  0.0251  0.0191  1.7516\n",
      "Model 2  0.0074  0.0065  0.4398\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with the evaluation results\n",
    "evaluation_data = {\n",
    "    'Model': ['Model 1', 'Model 2'],\n",
    "    'RMSE': [model1_rmse, model2_rmse],\n",
    "    'MAE': [model1_mae, model2_mae],\n",
    "    'MAPE': [model1_mape, model2_mape]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the evaluation data\n",
    "evaluation = pd.DataFrame(evaluation_data)\n",
    "\n",
    "# Set the 'Model' column as the index\n",
    "evaluation.set_index('Model', inplace=True)\n",
    "\n",
    "# Round the values to 4 decimal places\n",
    "evaluation = evaluation.round(4)\n",
    "\n",
    "# Print the evaluation DataFrame\n",
    "print(evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3b152",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Square Error) measures the prediction accuracy of the model by calculating the square root of the average of the squared differences between the predicted values and the actual values. The lower the RMSE value, the smaller the average deviation or error between the predictions and the actual values. In the context of stock prices, a low RMSE value indicates that the model's predictions tend to closely approximate the actual values accurately.\n",
    "\n",
    "MAE (Mean Absolute Error) measures the average absolute deviation between the predicted values and the actual values. MAE disregards the direction (positive or negative) of the prediction errors. The lower the MAE value, the closer the predictions are to the actual values overall. In the context of stock prices, a low MAE value indicates that the model generally has the ability to predict prices with low levels of error.\n",
    "\n",
    "MAPE (Mean Absolute Percentage Error) measures the average percentage of absolute errors relative to the actual values. MAPE provides an indication of how large the prediction errors are in proportion to the actual values. The lower the MAPE value, the smaller the percentage of prediction errors relative to the actual values. In the context of stock prices, a low MAPE value indicates that the model has a low level of error in predicting percentage changes in stock prices.\n",
    "\n",
    "Overall, the RMSE, MAE, and MAPE values of the second model are lower than those of the first model, indicating that the second model has better ability to capture patterns and trends in the data and minimize prediction errors compared to the first model. Therefore, the model that could be used by DJ Patil to help the company predict stock prices would be the second Transformers model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
